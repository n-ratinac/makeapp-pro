{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikol\\Documents\\projects\\makeapp-pro\\lab\\env\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import keypoints\n",
    "# Initialize MediaPipe Face Mesh.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "# Function to detect landmarks on a given image\n",
    "def detect_landmarks(image):\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if not results.multi_face_landmarks:\n",
    "        return None\n",
    "    landmarks = results.multi_face_landmarks[0]  # Assuming one face\n",
    "    return landmarks\n",
    "\n",
    "# Load initial and final images\n",
    "initial_image = cv2.imread(\"data/before/1.png\")\n",
    "final_image = cv2.imread(\"data/after/1.png\")\n",
    "# Detect landmarks\n",
    "initial_landmarks = detect_landmarks(initial_image)\n",
    "final_landmarks = detect_landmarks(final_image)\n",
    "\n",
    "if not initial_landmarks or not final_landmarks:\n",
    "    print(\"Could not detect landmarks in one of the images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the key landmarks (e.g., eyes, nose) for alignment\n",
    "def get_landmark_points(landmarks, image):\n",
    "    h, w, _ = image.shape\n",
    "    points = []\n",
    "    # Use specific landmark indices (for eyes and nose)\n",
    "    key_landmarks = keypoints.EYE_L3 + keypoints.EYE_R3 + keypoints.NOSE_TIP + keypoints.NOSE_BOTTOM + keypoints.CHEEK_LEFT + keypoints.CHEEK_RIGHT\n",
    "    for lm_index in key_landmarks:\n",
    "        x = int(landmarks.landmark[lm_index].x * w)\n",
    "        y = int(landmarks.landmark[lm_index].y * h)\n",
    "        points.append([x, y])\n",
    "    return np.array(points, dtype=np.float32)\n",
    "\n",
    "# Get corresponding points from both images\n",
    "initial_points = get_landmark_points(initial_landmarks, initial_image)\n",
    "final_points = get_landmark_points(final_landmarks, final_image)\n",
    "\n",
    "# Calculate the affine transformation matrix\n",
    "affine_matrix = cv2.getAffineTransform(initial_points[:3], final_points[:3])\n",
    "\n",
    "# Apply affine transformation to warp the initial image\n",
    "aligned_initial_image = cv2.warpAffine(initial_image, affine_matrix, (final_image.shape[1], final_image.shape[0]))\n",
    "\n",
    "# Show the aligned image\n",
    "cv2.imshow(\"Aligned Initial Image\", aligned_initial_image)\n",
    "cv2.imshow(\"Final Image\", final_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to compute similarity\n",
    "def compute_similarity(image1, image2):\n",
    "    # Convert both images to grayscale for comparison\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute absolute difference between images\n",
    "    difference = cv2.absdiff(gray1, gray2)\n",
    "\n",
    "    # Sum up the differences\n",
    "    similarity_score = np.sum(difference)\n",
    "    return similarity_score\n",
    "\n",
    "# Compute the similarity between aligned initial and final images\n",
    "similarity = compute_similarity(final_image, final_image)\n",
    "print(f\"Similarity Score: {similarity}\")\n",
    "\n",
    "def convert_to_hsv(image):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    return hsv_image\n",
    "\n",
    "def compute_histogram(image, mask=None):\n",
    "    # Compute histograms for Hue and Saturation channels (bins=50 for more granularity)\n",
    "    hsv_image = convert_to_hsv(image)\n",
    "    hist_hue = cv2.calcHist([hsv_image], [0], mask, [50], [0, 180])  # Hue channel\n",
    "    hist_saturation = cv2.calcHist([hsv_image], [1], mask, [50], [0, 256])  # Saturation channel\n",
    "\n",
    "    # Normalize histograms\n",
    "    cv2.normalize(hist_hue, hist_hue)\n",
    "    cv2.normalize(hist_saturation, hist_saturation)\n",
    "\n",
    "    return hist_hue, hist_saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to create a mask for a given region (based on landmarks)\n",
    "def create_region_mask(landmarks, image, region_points):\n",
    "    h, w, _ = image.shape\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Get the coordinates for the region\n",
    "    points = [(int(landmarks.landmark[p].x * w), int(landmarks.landmark[p].y * h)) for p in region_points]\n",
    "    \n",
    "    # Fill the polygon that represents the region\n",
    "    cv2.fillPoly(mask, [np.array(points)], 255)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Function to apply the mask to the image and black out the background\n",
    "def apply_mask(image, mask):\n",
    "    # Ensure mask is in 3 channels (for RGB image)\n",
    "    mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, mask_rgb)\n",
    "    return masked_image\n",
    "\n",
    "# Function to extract specific landmark points from the detected landmarks\n",
    "def get_landmark_points(landmarks, image, key_indices):\n",
    "    h, w, _ = image.shape\n",
    "    points = []\n",
    "    for idx in key_indices:\n",
    "        x = int(landmarks.landmark[idx].x * w)\n",
    "        y = int(landmarks.landmark[idx].y * h)\n",
    "        points.append([x, y])\n",
    "    return np.array(points, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 10359730\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\imgwarp.cpp:3631: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 3 && dst.checkVector(2, CV_32F) == 3 in function 'cv::getAffineTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m src_points \u001b[38;5;241m=\u001b[39m get_landmark_points(initial_landmarks, initial_image, keypoints\u001b[38;5;241m.\u001b[39mEYE_L3)\n\u001b[0;32m     13\u001b[0m tgt_points \u001b[38;5;241m=\u001b[39m get_landmark_points(final_landmarks, final_image, keypoints\u001b[38;5;241m.\u001b[39mEYE_L3)\n\u001b[1;32m---> 14\u001b[0m affine_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetAffineTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m aligned_initial_mask \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(src_mask, affine_matrix, (final_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], final_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     18\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAligned Initial Image\u001b[39m\u001b[38;5;124m\"\u001b[39m, aligned_initial_mask)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\imgwarp.cpp:3631: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 3 && dst.checkVector(2, CV_32F) == 3 in function 'cv::getAffineTransform'\n"
     ]
    }
   ],
   "source": [
    "# Create the mask for the lips region\n",
    "src_mask = create_region_mask(initial_landmarks, initial_image, keypoints.EYE_L3)\n",
    "tgt_mask = create_region_mask(final_landmarks, final_image, keypoints.EYE_L3)\n",
    "# Apply the mask to isolate the lips region\n",
    "src_eye_mask = apply_mask(initial_image, src_mask)\n",
    "tgt_eye_mask = apply_mask(final_image, tgt_mask)\n",
    "# Display the masked image\n",
    "\n",
    "#calculate similarity\n",
    "similarity = compute_similarity(tgt_eye_mask, src_eye_mask)\n",
    "print(f\"Similarity Score: {similarity}\")\n",
    "src_points = get_landmark_points(initial_landmarks, initial_image, keypoints.EYE_L3)\n",
    "tgt_points = get_landmark_points(final_landmarks, final_image, keypoints.EYE_L3)\n",
    "affine_matrix = cv2.getAffineTransform(src_points, tgt_points)\n",
    "aligned_initial_mask = cv2.warpAffine(src_mask, affine_matrix, (final_image.shape[1], final_image.shape[0]))\n",
    "\n",
    "\n",
    "cv2.imshow(\"Aligned Initial Image\", aligned_initial_mask)\n",
    "cv2.imshow(\"Final Image\", final_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
